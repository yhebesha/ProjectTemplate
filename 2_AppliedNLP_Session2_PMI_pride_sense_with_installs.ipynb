{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecedba6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install pandas matplotlib --quiet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f13ec8d5",
   "metadata": {},
   "source": [
    "# 2) PMI (Pointwise Mutual Information)\n",
    "\n",
    "This notebook is part of **Applied NLP – Session 2: Phrases & Collocations**.\n",
    "\n",
    "Overview:\n",
    "- Compute pointwise mutual information (PMI) for bigrams in two works by the same author.\n",
    "- Use PMI to surface word pairs that co-occur more often than expected by chance, then inspect frequency to avoid noise.\n",
    "\n",
    "Learning objectives:\n",
    "- Estimate unigram and bigram probabilities from counts and compute PMI.\n",
    "- Apply frequency thresholds to reduce spurious high-PMI rare pairs.\n",
    "- Visualize and export the top PMI bigrams as CSV and PNG for reporting.\n",
    "\n",
    "Quick start:\n",
    "1. Edit the `CONFIG` dictionary below to point to your two plain-text books.\n",
    "2. Optionally enable `use_stopwords` or provide a `STOPWORDS` set to remove function words.\n",
    "3. Run the notebook cells in order. Outputs are saved to `../results/PMI_table.csv` and `../results/PMI_figure.png`.\n",
    "\n",
    "Prerequisites:\n",
    "- A Python environment with requirements.txt requirements installed.\n",
    "- The two text files placed in `../data/`.\n",
    "\n",
    "Notes and tips:\n",
    "- PMI can highlight informative collocations but is sensitive to low counts. Use `min_ngram_count` to filter rare bigrams.\n",
    "- If you want consistent preprocessing across notebooks, copy the preprocessing cell from notebook 1 (Gutenberg stripping, curly-quote normalization, and the single-letter token prune).\n",
    "\n",
    "Tips from notebook 1:\n",
    "- The notebooks normalize curly quotes and prune single-letter tokens (except `a` and `i`) to avoid spurious n-grams such as `alice q s`.\n",
    "- If you run into unexpected tokens, run the preprocessing/inspection cells from notebook 1 to diagnose and repair encoding issues.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a516e09f",
   "metadata": {},
   "source": [
    "## 0. Setup & Configuration\n",
    "\n",
    "- Fill the `CONFIG` paths for your two books (plain text).\n",
    "- Toggle stopwords and thresholds as needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c24457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Imports & Config =====\n",
    "import re, os, math, json, collections\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (9, 4.5)\n",
    "plt.rcParams[\"axes.grid\"] = True\n",
    "\n",
    "CONFIG = {\n",
    "    \"book1_path\": \"../data/pride.txt\",  # <-- change\n",
    "    \"book2_path\": \"../data/sense.txt\",  # <-- change\n",
    "    \"language\": \"en\",                # e.g. 'en','de','ru','el'\n",
    "    \"use_stopwords\": False,          # toggle\n",
    "    \"min_ngram_count\": 5,            # threshold (where applicable)\n",
    "    \"top_k\": 20                      # top items to show\n",
    "}\n",
    "\n",
    "# Unicode-aware token regex: words with optional internal ' or -\n",
    "WORD_RE = re.compile(r\"[^\\W\\d_]+(?:[-'][^\\W\\d_]+)*\", flags=re.UNICODE)\n",
    "\n",
    "# Optional: supply your own stopwords set per language\n",
    "STOPWORDS = set()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "387bd2bd",
   "metadata": {},
   "source": [
    "## 1. Load & Normalize Text\n",
    "\n",
    "- Fix hyphenated line breaks (e.g., end-of-line hyphens).\n",
    "- Normalize whitespace.\n",
    "- Lowercase consistently.\n",
    "\n",
    "Our books are a part of Project Gutenberg, which means there are some extra texts in each txt file to be cleaned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8f8ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Robust Project Gutenberg boilerplate stripper --------------------------\n",
    "_GB_START_MARKERS = [\n",
    "    r\"\\*\\*\\*\\s*START OF (THIS|THE) PROJECT GUTENBERG EBOOK\",   # modern\n",
    "    r\"START OF (THIS|THE) PROJECT GUTENBERG EBOOK\",             # fallback\n",
    "    r\"End of the Project Gutenberg(?:'s)? Etext\",               # very old variants sometimes inverted\n",
    "]\n",
    "_GB_END_MARKERS = [\n",
    "    r\"\\*\\*\\*\\s*END OF (THIS|THE) PROJECT GUTENBERG EBOOK\",      # modern\n",
    "    r\"END OF (THIS|THE) PROJECT GUTENBERG EBOOK\",                # fallback\n",
    "    r\"End of Project Gutenberg(?:'s)? (?:Etext|eBook)\",          # older variants\n",
    "    r\"\\*\\*\\*\\s*END: FULL LICENSE\\s*\\*\\*\\*\",                      # license block end (older)\n",
    "]\n",
    "\n",
    "# Chapters (heuristic fallback if markers missing; English-centric but works often)\n",
    "_CHAPTER_HINTS = [\n",
    "    r\"^\\s*chapter\\s+[ivxlcdm0-9]+[\\.\\: ]\",   # CHAPTER I / Chapter 1\n",
    "    r\"^\\s*book\\s+[ivxlcdm0-9]+[\\.\\: ]\",      # BOOK I etc.\n",
    "    r\"^\\s*part\\s+[ivxlcdm0-9]+[\\.\\: ]\",\n",
    "]\n",
    "\n",
    "def strip_gutenberg(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Returns text between Gutenberg START and END markers (case-insensitive).\n",
    "    If markers aren't found, heuristically trims to first chapter-like heading.\n",
    "    Works for most EN/DE/RU/EL releases since headers are in English.\n",
    "    \"\"\"\n",
    "    t = text.replace(\"\\ufeff\", \"\")  # strip BOM if present\n",
    "\n",
    "    # Find START\n",
    "    start_idx = None\n",
    "    for pat in _GB_START_MARKERS:\n",
    "        m = re.search(pat, t, flags=re.IGNORECASE)\n",
    "        if m:\n",
    "            # start AFTER the matched line\n",
    "            start_idx = t.find(\"\\n\", m.end())\n",
    "            if start_idx == -1:\n",
    "                start_idx = m.end()\n",
    "            break\n",
    "\n",
    "    # Find END\n",
    "    end_idx = None\n",
    "    for pat in _GB_END_MARKERS:\n",
    "        m = re.search(pat, t, flags=re.IGNORECASE)\n",
    "        if m:\n",
    "            # end BEFORE the matched line\n",
    "            end_idx = m.start()\n",
    "            break\n",
    "\n",
    "    if start_idx is not None and end_idx is not None and end_idx > start_idx:\n",
    "        core = t[start_idx:end_idx]\n",
    "    else:\n",
    "        # Fallback: try to start at first chapter-like heading\n",
    "        core = t\n",
    "        for pat in _CHAPTER_HINTS:\n",
    "            m = re.search(pat, core, flags=re.IGNORECASE | re.MULTILINE)\n",
    "            if m:\n",
    "                core = core[m.start():]\n",
    "                break\n",
    "        # And trim off the standard license tail if present\n",
    "        for pat in _GB_END_MARKERS:\n",
    "            m = re.search(pat, core, flags=re.IGNORECASE)\n",
    "            if m:\n",
    "                core = core[:m.start()]\n",
    "                break\n",
    "\n",
    "    # Remove license/contact blocks that sometimes sneak inside\n",
    "    core = re.sub(r\"\\n\\s*End of the Project Gutenberg.*\", \"\", core, flags=re.IGNORECASE)\n",
    "    core = re.sub(r\"\\*\\*\\*\\s*START: FULL LICENSE\\s*\\*\\*\\*.*\", \"\", core, flags=re.IGNORECASE | re.DOTALL)\n",
    "\n",
    "    # Clean leftover cruft: URLs, repeated separators\n",
    "    core = re.sub(r\"https?://\\S+\", \"\", core)\n",
    "    core = re.sub(r\"[ \\t]+\\n\", \"\\n\", core)   # trailing spaces before newline\n",
    "    core = re.sub(r\"\\n{3,}\", \"\\n\\n\", core)   # collapse big blank blocks\n",
    "    return core.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a2d52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_text(p: str) -> str:\n",
    "    with open(p, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        return f.read()\n",
    "\n",
    "def normalize_text(t: str) -> str:\n",
    "    # 1) strip Gutenberg header/footer FIRST\n",
    "    # This removes license text and front/back matter that would otherwise pollute counts.\n",
    "    t = strip_gutenberg(t)\n",
    "    # 1.5) Normalize smart/curly apostrophes to ASCII apostrophe so contractions\n",
    "    # (e.g., don’t) are preserved as single tokens rather than split into\n",
    "    # two tokens (don t). This handles U+2019 (right single quotation mark) and\n",
    "    # U+2018 (left single quotation mark) commonly found in some texts/encodings.\n",
    "    t = t.replace(\"’\", \"'\")\n",
    "    t = t.replace(\"‘\", \"'\")\n",
    "    # 2) join hyphenated line breaks (e.g., \"won-\\nderful\")\n",
    "    t = re.sub(r\"-\\s*\\n\", \"\", t)\n",
    "    # 3) normalize whitespace\n",
    "    t = re.sub(r\"\\s+\", \" \", t)\n",
    "    return t\n",
    "\n",
    "text1 = normalize_text(load_text(CONFIG[\"book1_path\"]))\n",
    "text2 = normalize_text(load_text(CONFIG[\"book2_path\"]))\n",
    "\n",
    "tokens1 = WORD_RE.findall(text1.lower())\n",
    "tokens2 = WORD_RE.findall(text2.lower())\n",
    "\n",
    "if CONFIG[\"use_stopwords\"]:\n",
    "    tokens1 = [t for t in tokens1 if t not in STOPWORDS]\n",
    "    tokens2 = [t for t in tokens2 if t not in STOPWORDS]\n",
    "\n",
    "tokens = tokens1 + tokens2\n",
    "\n",
    "len(tokens1), len(tokens2), len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a11313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Diagnostic: list most frequent 1- and 2-letter tokens and show raw contexts\n",
    "# from collections import Counter\n",
    "# short_counts = Counter(t for t in tokens if len(t) <= 2)\n",
    "# print('Top short tokens (len<=2):')\n",
    "# for tok, c in short_counts.most_common(30):\n",
    "#     print(f'{tok!r}: {c}')\n",
    "\n",
    "# # Show example contexts for top few short tokens\n",
    "# def show_contexts(tok, max_examples=5, window=30):\n",
    "#     examples = []\n",
    "#     raw = load_text(CONFIG['book1_path']) + '\\n---BOOK2---\\n' + load_text(CONFIG['book2_path'])\n",
    "#     # find in token sequence positions in combined tokens and map to approximate char positions by naive search\n",
    "#     joined = ' '.join(tokens)\n",
    "#     start = 0\n",
    "#     found = 0\n",
    "#     for m in re.finditer(re.escape(tok), joined):\n",
    "#         if found >= max_examples: break\n",
    "#         pos = m.start()\n",
    "#         # approximate slice from joined tokens may be enough for inspection\n",
    "#         s = joined[max(0,pos-window): pos+window]\n",
    "#         print('  context:', repr(s))\n",
    "#         found += 1\n",
    "#     if found == 0:\n",
    "#         print('  (no simple joined-token context found)')\n",
    "\n",
    "# for tok, _ in short_counts.most_common(10):\n",
    "#     print('\\nExamples for token:', tok)\n",
    "#     show_contexts(tok)\n",
    "\n",
    "# print('Diagnostic complete. If a short token is spurious (e.g., \"q\", \"s\", \"th\"), consider pruning it.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c3712c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prune short tokens: keep only 'a' and 'i' for length 1, and a whitelist for length 2\n",
    "keep_1 = {'a', 'i'}\n",
    "# Common 2-letter English words to keep; extend if you need more\n",
    "keep_2 = {'of','to','in','on','by','an','or','as','is','it','we','us','he','me','my','so','be','do','no','at','up','if','go','am','oh'}\n",
    "\n",
    "# Apply pruning to per-book tokens if present\n",
    "if 'tokens1' in globals() and 'tokens2' in globals():\n",
    "    tokens1 = [t for t in tokens1 if (len(t) > 2) or (len(t) == 1 and t in keep_1) or (len(t) == 2 and t in keep_2)]\n",
    "    tokens2 = [t for t in tokens2 if (len(t) > 2) or (len(t) == 1 and t in keep_1) or (len(t) == 2 and t in keep_2)]\n",
    "    tokens = tokens1 + tokens2\n",
    "else:\n",
    "    tokens = [t for t in tokens if (len(t) > 2) or (len(t) == 1 and t in keep_1) or (len(t) == 2 and t in keep_2)]\n",
    "\n",
    "print('After pruning: counts ->', 'tokens1=' + str(len(tokens1)) if 'tokens1' in globals() else '', 'tokens=' + str(len(tokens)))\n",
    "# Show remaining short tokens (sanity check)\n",
    "from collections import Counter\n",
    "sc = Counter(t for t in tokens if len(t) <= 2)\n",
    "print('Remaining short tokens:', sc.most_common(30))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10e2c50",
   "metadata": {},
   "source": [
    "## 2. Unigram & Bigram Counts\n",
    "\n",
    "We estimate probabilities from observed counts, then compute PMI:\n",
    "\n",
    "\\[ \\text{PMI}(w_i, w_{i+1}) = \\log_2 \\frac{p(w_i, w_{i+1})}{p(w_i)\\,p(w_{i+1})} \\]\n",
    "\n",
    "To reduce noise, filter out rare bigrams with `min_ngram_count`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661656a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams = Counter(tokens)\n",
    "bigrams = Counter(zip(tokens, tokens[1:]))\n",
    "\n",
    "N1 = sum(unigrams.values())\n",
    "N2 = sum(bigrams.values())\n",
    "\n",
    "min_c = CONFIG[\"min_ngram_count\"]\n",
    "bigrams_f = {bg:c for bg,c in bigrams.items() if c >= min_c}\n",
    "\n",
    "def pmi(a, b):\n",
    "    pa = unigrams[a] / N1 if N1 else 0\n",
    "    pb = unigrams[b] / N1 if N1 else 0\n",
    "    pab = bigrams[(a,b)] / N2 if N2 else 0\n",
    "    if pa <= 0 or pb <= 0 or pab <= 0:\n",
    "        return float(\"-inf\")\n",
    "    return math.log2(pab / (pa * pb))\n",
    "\n",
    "rows = []\n",
    "for (a,b), c in bigrams_f.items():\n",
    "    rows.append({\"bigram\": f\"{a} {b}\", \"count\": c, \"PMI\": pmi(a,b)})\n",
    "\n",
    "pmi_df = (pd.DataFrame(rows)\n",
    "          .replace([float(\"inf\"), float(\"-inf\")], pd.NA)\n",
    "          .dropna()\n",
    "          .sort_values([\"PMI\",\"count\"], ascending=[False, False])\n",
    "          .head(CONFIG[\"top_k\"]))\n",
    "pmi_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9261a3",
   "metadata": {},
   "source": [
    "## 3. Visualize Top PMI Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addf94a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = pmi_df.plot.barh(x=\"bigram\", y=\"PMI\", legend=False)\n",
    "ax.invert_yaxis()\n",
    "ax.set_title(f\"Top PMI Bigrams (min count ≥ {min_c})\")\n",
    "# Keep a reference to the Figure so we can save it reliably later (avoids empty canvas)\n",
    "fig_pmi = ax.get_figure()\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c4d510",
   "metadata": {},
   "source": [
    "## 4. Notes\n",
    "\n",
    "- High PMI indicates stronger-than-chance association.\n",
    "- Inspect frequency too; very rare but high-PMI can still be noise.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d205fa9",
   "metadata": {},
   "source": [
    "## 5. Reflection (Answer in your repo's README or below)\n",
    "\n",
    "- Which results matched your reading intuition?\n",
    "- What surprised you?\n",
    "- If you toggled preprocessing (stopwords on/off), what changed?\n",
    "- Compare across the two works: are the patterns stable?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ab79a5",
   "metadata": {},
   "source": [
    "## 6. Export (tables/figures)\n",
    "\n",
    "This cell saves outputs into the `../results/` folder so you can add them to your report.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78320848",
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(\"../results\").mkdir(exist_ok=True)\n",
    "pmi_df.to_csv(f\"../results/PMI_table.csv\", index=False)\n",
    "# Prefer saving via the captured Figure object to avoid backend/display issues.\n",
    "try:\n",
    "    fig_pmi.savefig(f\"../results/PMI_figure.png\", dpi=200, bbox_inches=\"tight\")\n",
    "except NameError:\n",
    "    # fig_pmi not defined (plot cell may not have been run); fall back to pyplot save\n",
    "    try:\n",
    "        plt.savefig(f\"../results/PMI_figure.png\", dpi=200, bbox_inches=\"tight\")\n",
    "    except Exception:\n",
    "        pass\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
