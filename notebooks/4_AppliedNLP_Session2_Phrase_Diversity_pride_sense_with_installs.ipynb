{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e629c282",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install pandas matplotlib --quiet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6e32df",
   "metadata": {},
   "source": [
    "# 4) Phrase Diversity (n-gram TTR)\n",
    "\n",
    "This notebook is part of **Applied NLP – Session 2: Phrases & Collocations**.\n",
    "\n",
    "Overview:\n",
    "- Measure phrase diversity using Type-Token Ratio (TTR) for bigrams and trigrams.\n",
    "- Compare lexical variety across two works by the same author.\n",
    "- (Optional) Analyze diversity trends across sections/chapters within the texts.\n",
    "\n",
    "Learning objectives:\n",
    "- Understand and compute Type-Token Ratio (TTR) as a measure of lexical/phrasal diversity.\n",
    "- Apply TTR to n-grams (bigrams, trigrams) to quantify phrase variety.\n",
    "- Interpret TTR values: lower TTR suggests formulaic/repetitive phrasing, higher TTR indicates more varied expression.\n",
    "- Produce reproducible diversity metrics and visualizations for literary analysis.\n",
    "\n",
    "Quick start:\n",
    "1. Edit the `CONFIG` dictionary in the next code cell to point to your two plain-text books.\n",
    "2. (Optional) Toggle `use_stopwords` to exclude common function words.\n",
    "3. Run cells from top to bottom. The main outputs are saved to `../results/`.\n",
    "4. (Optional) Customize section-splitting regex to analyze diversity by chapter.\n",
    "\n",
    "Prerequisites:\n",
    "- A Python environment with requirements.txt packages installed (pandas, matplotlib).\n",
    "- The text files for the two works placed in `../data/`.\n",
    "\n",
    "Notes and tips:\n",
    "- The notebook uses the same robust preprocessing as notebooks 1-2 (strip_gutenberg, normalize quotes, etc.).\n",
    "- TTR formula: (unique n-grams) / (total n-grams). Values range from 0 to 1.\n",
    "- Lower TTR: more repetition (e.g., epic poetry with formulaic epithets like \"wine-dark sea\").\n",
    "- Higher TTR: more varied phrasing (e.g., experimental modernist prose).\n",
    "- Section-wise analysis (optional cell) helps identify where diversity changes within a work.\n",
    "- Compare TTR between your two books to see if the author's style evolved or differs by genre.\n",
    "- Stopword removal may artificially inflate TTR by reducing common bigrams like \"of the\" or \"in the\".\n",
    "\n",
    "**Goal:** Apply Type-Token Ratio (TTR) to bigrams and trigrams to measure phrase diversity in your two selected works.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ff57e43",
   "metadata": {},
   "source": [
    "## 0. Setup & Configuration\n",
    "\n",
    "- Fill the `CONFIG` paths for your two books (plain text).\n",
    "- Toggle stopwords and thresholds as needed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85f2a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Imports & Config =====\n",
    "import re, os, math, json, collections\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (9, 4.5)\n",
    "plt.rcParams[\"axes.grid\"] = True\n",
    "\n",
    "CONFIG = {\n",
    "    \"book1_path\": \"../data/pride.txt\",  # <-- change\n",
    "    \"book2_path\": \"../data/sense.txt\",  # <-- change\n",
    "    \"language\": \"en\",                # e.g. 'en','de','ru','el'\n",
    "    \"use_stopwords\": False,          # toggle\n",
    "    \"min_ngram_count\": 5,            # threshold (where applicable)\n",
    "    \"top_k\": 20                      # top items to show\n",
    "}\n",
    "\n",
    "# Unicode-aware token regex: words with optional internal ' or -\n",
    "WORD_RE = re.compile(r\"[^\\W\\d_]+(?:[-'][^\\W\\d_]+)*\", flags=re.UNICODE)\n",
    "\n",
    "# Optional: supply your own stopwords set per language\n",
    "STOPWORDS = set()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456952dc",
   "metadata": {},
   "source": [
    "## 1. Load & Normalize Text\n",
    "\n",
    "- Fix hyphenated line breaks (e.g., end-of-line hyphens).\n",
    "- Normalize whitespace.\n",
    "- Lowercase consistently.\n",
    "\n",
    "Our books are a part of Project Gutenberg, which means there are some extra texts in each txt file to be cleaned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a861df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Robust Project Gutenberg boilerplate stripper --------------------------\n",
    "_GB_START_MARKERS = [\n",
    "    r\"\\*\\*\\*\\s*START OF (THIS|THE) PROJECT GUTENBERG EBOOK\",   # modern\n",
    "    r\"START OF (THIS|THE) PROJECT GUTENBERG EBOOK\",             # fallback\n",
    "    r\"End of the Project Gutenberg(?:'s)? Etext\",               # very old variants sometimes inverted\n",
    "]\n",
    "_GB_END_MARKERS = [\n",
    "    r\"\\*\\*\\*\\s*END OF (THIS|THE) PROJECT GUTENBERG EBOOK\",      # modern\n",
    "    r\"END OF (THIS|THE) PROJECT GUTENBERG EBOOK\",                # fallback\n",
    "    r\"End of Project Gutenberg(?:'s)? (?:Etext|eBook)\",          # older variants\n",
    "    r\"\\*\\*\\*\\s*END: FULL LICENSE\\s*\\*\\*\\*\",                      # license block end (older)\n",
    "]\n",
    "\n",
    "# Chapters (heuristic fallback if markers missing; English-centric but works often)\n",
    "_CHAPTER_HINTS = [\n",
    "    r\"^\\s*chapter\\s+[ivxlcdm0-9]+[\\.\\: ]\",   # CHAPTER I / Chapter 1\n",
    "    r\"^\\s*book\\s+[ivxlcdm0-9]+[\\.\\: ]\",      # BOOK I etc.\n",
    "    r\"^\\s*part\\s+[ivxlcdm0-9]+[\\.\\: ]\",\n",
    "]\n",
    "\n",
    "def strip_gutenberg(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Returns text between Gutenberg START and END markers (case-insensitive).\n",
    "    If markers aren't found, heuristically trims to first chapter-like heading.\n",
    "    Works for most EN/DE/RU/EL releases since headers are in English.\n",
    "    \"\"\"\n",
    "    t = text.replace(\"\\ufeff\", \"\")  # strip BOM if present\n",
    "\n",
    "    # Find START\n",
    "    start_idx = None\n",
    "    for pat in _GB_START_MARKERS:\n",
    "        m = re.search(pat, t, flags=re.IGNORECASE)\n",
    "        if m:\n",
    "            # start AFTER the matched line\n",
    "            start_idx = t.find(\"\\n\", m.end())\n",
    "            if start_idx == -1:\n",
    "                start_idx = m.end()\n",
    "            break\n",
    "\n",
    "    # Find END\n",
    "    end_idx = None\n",
    "    for pat in _GB_END_MARKERS:\n",
    "        m = re.search(pat, t, flags=re.IGNORECASE)\n",
    "        if m:\n",
    "            # end BEFORE the matched line\n",
    "            end_idx = m.start()\n",
    "            break\n",
    "\n",
    "    if start_idx is not None and end_idx is not None and end_idx > start_idx:\n",
    "        core = t[start_idx:end_idx]\n",
    "    else:\n",
    "        # Fallback: try to start at first chapter-like heading\n",
    "        core = t\n",
    "        for pat in _CHAPTER_HINTS:\n",
    "            m = re.search(pat, core, flags=re.IGNORECASE | re.MULTILINE)\n",
    "            if m:\n",
    "                core = core[m.start():]\n",
    "                break\n",
    "        # And trim off the standard license tail if present\n",
    "        for pat in _GB_END_MARKERS:\n",
    "            m = re.search(pat, core, flags=re.IGNORECASE)\n",
    "            if m:\n",
    "                core = core[:m.start()]\n",
    "                break\n",
    "\n",
    "    # Remove license/contact blocks that sometimes sneak inside\n",
    "    core = re.sub(r\"\\n\\s*End of the Project Gutenberg.*\", \"\", core, flags=re.IGNORECASE)\n",
    "    core = re.sub(r\"\\*\\*\\*\\s*START: FULL LICENSE\\s*\\*\\*\\*.*\", \"\", core, flags=re.IGNORECASE | re.DOTALL)\n",
    "\n",
    "    # Clean leftover cruft: URLs, repeated separators\n",
    "    core = re.sub(r\"https?://\\S+\", \"\", core)\n",
    "    core = re.sub(r\"[ \\t]+\\n\", \"\\n\", core)   # trailing spaces before newline\n",
    "    core = re.sub(r\"\\n{3,}\", \"\\n\\n\", core)   # collapse big blank blocks\n",
    "    return core.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30a5630",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_text(p: str) -> str:\n",
    "    with open(p, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        return f.read()\n",
    "\n",
    "def normalize_text(t: str) -> str:\n",
    "    # 1) strip Gutenberg header/footer FIRST\n",
    "    t = strip_gutenberg(t)\n",
    "    # 2) join hyphenated line breaks (e.g., \"won-\\nderful\")\n",
    "    t = re.sub(r\"-\\s*\\n\", \"\", t)\n",
    "    # 3) normalize whitespace\n",
    "    t = re.sub(r\"\\s+\", \" \", t)\n",
    "    return t\n",
    "\n",
    "text1 = normalize_text(load_text(CONFIG[\"book1_path\"]))\n",
    "text2 = normalize_text(load_text(CONFIG[\"book2_path\"]))\n",
    "\n",
    "tokens1 = WORD_RE.findall(text1.lower())\n",
    "tokens2 = WORD_RE.findall(text2.lower())\n",
    "\n",
    "if CONFIG[\"use_stopwords\"]:\n",
    "    tokens1 = [t for t in tokens1 if t not in STOPWORDS]\n",
    "    tokens2 = [t for t in tokens2 if t not in STOPWORDS]\n",
    "\n",
    "tokens = tokens1 + tokens2\n",
    "\n",
    "len(tokens1), len(tokens2), len(tokens)\n",
    "\n",
    "len(tokens), tokens[:12]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e3bc71",
   "metadata": {},
   "source": [
    "## 2. Compute TTR for Bigrams & Trigrams\n",
    "\n",
    "Type–Token Ratio for n-grams measures phrase variety.\n",
    "\n",
    "$$\\text{TTR}(S) = \\frac{|\\text{unique n-grams}|}{|\\text{total n-grams}|}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da4d4191",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ttr(seq):\n",
    "    return len(set(seq)) / max(1, len(seq))\n",
    "\n",
    "# Combined corpus TTR\n",
    "bigrams_list = list(zip(tokens, tokens[1:]))\n",
    "trigrams_list = list(zip(tokens, tokens[1:], tokens[2:]))\n",
    "\n",
    "ttr2_combined = ttr(bigrams_list)\n",
    "ttr3_combined = ttr(trigrams_list)\n",
    "\n",
    "# Book 1 TTR\n",
    "bigrams_list_1 = list(zip(tokens1, tokens1[1:]))\n",
    "trigrams_list_1 = list(zip(tokens1, tokens1[1:], tokens1[2:]))\n",
    "\n",
    "ttr2_book1 = ttr(bigrams_list_1)\n",
    "ttr3_book1 = ttr(trigrams_list_1)\n",
    "\n",
    "# Book 2 TTR\n",
    "bigrams_list_2 = list(zip(tokens2, tokens2[1:]))\n",
    "trigrams_list_2 = list(zip(tokens2, tokens2[1:], tokens2[2:]))\n",
    "\n",
    "ttr2_book2 = ttr(bigrams_list_2)\n",
    "ttr3_book2 = ttr(trigrams_list_2)\n",
    "\n",
    "# Create comparison DataFrame\n",
    "summary_df = pd.DataFrame({\n",
    "    \"metric\": [\"bigram_TTR\", \"trigram_TTR\"],\n",
    "    \"combined\": [ttr2_combined, ttr3_combined],\n",
    "    \"book1\": [ttr2_book1, ttr3_book1],\n",
    "    \"book2\": [ttr2_book2, ttr3_book2]\n",
    "})\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9a094f",
   "metadata": {},
   "source": [
    "## 2.1 Visualize TTR Comparison\n",
    "\n",
    "Compare bigram and trigram diversity visually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0a0acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "fig_ttr, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Set up grouped bar chart\n",
    "x = np.arange(len(summary_df[\"metric\"]))\n",
    "width = 0.25\n",
    "\n",
    "# Create bars for each book\n",
    "bars1 = ax.bar(x - width, summary_df[\"book1\"], width, label='Book 1', color='steelblue', alpha=0.8)\n",
    "bars2 = ax.bar(x, summary_df[\"book2\"], width, label='Book 2', color='coral', alpha=0.8)\n",
    "bars3 = ax.bar(x + width, summary_df[\"combined\"], width, label='Combined', color='seagreen', alpha=0.8)\n",
    "\n",
    "# Customize plot\n",
    "ax.set_ylabel(\"TTR Score\", fontsize=12)\n",
    "ax.set_xlabel(\"N-gram Type\", fontsize=12)\n",
    "ax.set_title(\"Phrase Diversity Comparison: Type-Token Ratio\", fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(summary_df[\"metric\"])\n",
    "ax.legend(fontsize=10)\n",
    "ax.set_ylim(0, max(summary_df[[\"book1\", \"book2\", \"combined\"]].max()) * 1.15)\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bars in [bars1, bars2, bars3]:\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                f'{height:.4f}',\n",
    "                ha='center', va='bottom', fontsize=9, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031f955f",
   "metadata": {},
   "source": [
    "## 3. (Optional) Section-wise Diversity\n",
    "\n",
    "If chapters/sections are detectable by regex, estimate diversity per section.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6289e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # naive split by keywords; customize per book\n",
    "# sections = re.split(r\"\\bchapter\\b|\\bbook\\b|\\bpart\\b\", full_text, flags=re.IGNORECASE)\n",
    "# rows = []\n",
    "# for i, sec in enumerate(sections, start=1):\n",
    "#     toks = WORD_RE.findall(sec.lower())\n",
    "#     b2 = list(zip(toks, toks[1:]))\n",
    "#     b3 = list(zip(toks, toks[1:], toks[2:]))\n",
    "#     rows.append({\"section\": i, \"bigram_TTR\": ttr(b2), \"trigram_TTR\": ttr(b3)})\n",
    "# sec_df = pd.DataFrame(rows)\n",
    "\n",
    "# ax = sec_df.plot(x=\"section\", y=[\"bigram_TTR\",\"trigram_TTR\"])\n",
    "# ax.set_title(\"Phrase Diversity by Section (proxy)\")\n",
    "# ax.set_xlabel(\"Section index\")\n",
    "# ax.set_ylabel(\"TTR\")\n",
    "# plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0853b1a0",
   "metadata": {},
   "source": [
    "## 4. Notes\n",
    "\n",
    "- Lower TTR suggests more formulaic phrasing (e.g., epic epithets).\n",
    "- Compare the two works separately if you can split cleanly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ebc85b",
   "metadata": {},
   "source": [
    "## 5. Reflection (Answer in your repo's README or below)\n",
    "\n",
    "- Which results matched your reading intuition?\n",
    "- What surprised you?\n",
    "- If you toggled preprocessing (stopwords on/off), what changed?\n",
    "- Compare across the two works: are the patterns stable?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea5f239",
   "metadata": {},
   "source": [
    "## 6. Export (tables/figures)\n",
    "\n",
    "This cell saves outputs into the `../results/` folder so you can add them to your report.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7ef385d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(\"../results\").mkdir(exist_ok=True)\n",
    "\n",
    "# Save CSV table\n",
    "summary_df.to_csv(\"../results/TTR_table.csv\", index=False)\n",
    "\n",
    "# Save figure explicitly by referencing the Figure object we created earlier\n",
    "try:\n",
    "    fig_ttr.savefig(\"../results/TTR_figure.png\", dpi=200, bbox_inches=\"tight\")\n",
    "    print(\"✓ Saved TTR_table.csv and TTR_figure.png to ../results/\")\n",
    "except NameError:\n",
    "    # fig_ttr not defined (visualization cell may not have been executed)\n",
    "    print(\"⚠ Figure not saved - run the visualization cell first\")\n",
    "    try:\n",
    "        plt.savefig(\"../results/TTR_figure.png\", dpi=200, bbox_inches=\"tight\")\n",
    "    except Exception:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae30dc8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
