{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202b9516",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install pandas matplotlib --quiet\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d168ea3",
   "metadata": {},
   "source": [
    "# 1) Bigrams & Trigrams (Frequency & Comparison)\n",
    "\n",
    "This notebook is part of **Applied NLP – Session 2: Phrases & Collocations**.\n",
    "\n",
    "Overview:\n",
    "- Compute and compare frequent bigrams and trigrams from two works by the same author. All examples are running on Alice in wonderland and Alice through the looking glass books.\n",
    "- Visualize both the most frequent phrases and those that are most distinctive for each work.\n",
    "\n",
    "Learning objectives:\n",
    "- Practice robust text preprocessing for Project Gutenberg texts (strip headers, normalize quotes, fix hyphenation). You should be able to modify these cleaning steps for your own use cases.\n",
    "- Generate n-grams, compute normalized rates, and compare phrase distributions across texts.\n",
    "- Produce reproducible CSV and PNG artifacts under `../results/` for reports or slides later.\n",
    "\n",
    "Quick start:\n",
    "1. Edit the `CONFIG` dictionary in the next code cell to point to your two plain-text books.\n",
    "2. (Optional) Toggle `use_stopwords` to remove common function words like \"the\" or \"and\".\n",
    "3. Run cells from top to bottom. The main outputs are saved to `../results/`.\n",
    "\n",
    "Prerequisites:\n",
    "- A Python environment with requirements.txt requirements installed.\n",
    "- The text files for the two works placed in `../data/`.\n",
    "\n",
    "Notes and tips:\n",
    "- The notebook normalizes curly quotes and prunes single-letter tokens (except `a` and `i`) to avoid spurious n-grams like `alice q s`.\n",
    "- If you see unexpected tokens, inspect where they are coming from in the raw text with some inspection cells and find your own solution to tackle the issue.\n",
    "\n",
    "**Goal:** Compute frequent **bigrams** and **trigrams** for your two selected works, compare across books, and visualize the most characteristic phrases.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70e1c7c",
   "metadata": {},
   "source": [
    "## 0. Setup & Configuration\n",
    "\n",
    "- Fill the `CONFIG` paths for your two books (plain text).\n",
    "- Toggle stopwords and thresholds as needed.\n",
    "- Results are saved under `../results/`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6a9b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Imports & Config =====\n",
    "import re, os, math, json, collections\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (9, 4.5)\n",
    "plt.rcParams[\"axes.grid\"] = True\n",
    "\n",
    "CONFIG = {\n",
    "    \"book1_path\": \"../data/pride.txt\",  # <-- change\n",
    "    \"book2_path\": \"../data/sense.txt\",  # <-- change\n",
    "    \"language\": \"en\",                # e.g. 'en','de','ru','el'\n",
    "    \"use_stopwords\": True,          # toggle\n",
    "    \"min_ngram_count\": 5,            # threshold (where applicable)\n",
    "    \"top_k\": 20,                      # top items to show\n",
    "    \"rate_base\": 10000               # per-N tokens for normalized rates\n",
    "}\n",
    "\n",
    "# Unicode-aware token regex: words with optional internal ' or -\n",
    "# Keeps contractions (e.g., don't) and hyphenated compounds (e.g., well-known);\n",
    "# intentionally excludes digits and underscores.\n",
    "# Allow both ASCII apostrophe (') and common curly apostrophe (’) inside tokens\n",
    "WORD_RE = re.compile(r\"[^\\W\\d_]+(?:[-'\\u2019][^\\W\\d_]+)*\", flags=re.UNICODE)\n",
    "\n",
    "\n",
    "# Use spaCy English stop words (preferred for English-only runs).\n",
    "# This does not require downloading a language model; spaCy exposes a built-in\n",
    "# stop word set under spacy.lang.en.stop_words. If spaCy is not installed,\n",
    "# fall back to a small built-in set so the notebook still runs offline.\n",
    "try:\n",
    "    import spacy\n",
    "    from spacy.lang.en.stop_words import STOP_WORDS as _spacy_stop\n",
    "    STOPWORDS = {w.lower() for w in _spacy_stop}\n",
    "    stop_source = 'spacy'\n",
    "except Exception:\n",
    "    # spaCy not available; use a conservative built-in set to keep things running\n",
    "    STOPWORDS = {\n",
    "        'the','and','to','of','a','in','it','is','that','i','you','he','she',\n",
    "        'they','we','was','for','on','with','as','at','by','an'\n",
    "    }\n",
    "# Merge optional extras from CONFIG and sensible fiction defaults (lowercased)\n",
    "EXTRA = set(CONFIG.get('extra_stopwords', []))\n",
    "EXTRA = {x.lower() for x in EXTRA}\n",
    "EXTRA |= {'said'}\n",
    "STOPWORDS |= EXTRA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9edf89c3",
   "metadata": {},
   "source": [
    "## 1. Load & Normalize Text\n",
    "\n",
    "- Fix hyphenated line breaks (e.g., end-of-line hyphens).\n",
    "- Normalize whitespace.\n",
    "- Lowercase consistently.\n",
    "\n",
    "Our books are a part of Project Gutenberg, which means there are some extra texts in each txt file to be cleaned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b77de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Robust Project Gutenberg boilerplate stripper --------------------------\n",
    "_GB_START_MARKERS = [\n",
    "    r\"\\*\\*\\*\\s*START OF (THIS|THE) PROJECT GUTENBERG EBOOK\",   # modern\n",
    "    r\"START OF (THIS|THE) PROJECT GUTENBERG EBOOK\",             # fallback\n",
    "    r\"End of the Project Gutenberg(?:'s)? Etext\",               # very old variants sometimes inverted\n",
    "]\n",
    "_GB_END_MARKERS = [\n",
    "    r\"\\*\\*\\*\\s*END OF (THIS|THE) PROJECT GUTENBERG EBOOK\",      # modern\n",
    "    r\"END OF (THIS|THE) PROJECT GUTENBERG EBOOK\",                # fallback\n",
    "    r\"End of Project Gutenberg(?:'s)? (?:Etext|eBook)\",          # older variants\n",
    "    r\"\\*\\*\\*\\s*END: FULL LICENSE\\s*\\*\\*\\*\",                      # license block end (older)\n",
    "]\n",
    "\n",
    "# Chapters (heuristic fallback if markers missing; English-centric but works often)\n",
    "_CHAPTER_HINTS = [\n",
    "    r\"^\\s*chapter\\s+[ivxlcdm0-9]+[\\.\\: ]\",   # CHAPTER I / Chapter 1\n",
    "    r\"^\\s*book\\s+[ivxlcdm0-9]+[\\.\\: ]\",      # BOOK I etc.\n",
    "    r\"^\\s*part\\s+[ivxlcdm0-9]+[\\.\\: ]\",\n",
    "]\n",
    "\n",
    "def strip_gutenberg(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Returns text between Gutenberg START and END markers (case-insensitive).\n",
    "    If markers aren't found, heuristically trims to first chapter-like heading.\n",
    "    Works for most EN/DE/RU/EL releases since headers are in English.\n",
    "\n",
    "    Usage: call `strip_gutenberg(text)` before tokenization to remove header/footer noise.\n",
    "    \"\"\"\n",
    "    t = text.replace(\"\\ufeff\", \"\")  # strip BOM if present\n",
    "\n",
    "    # Find START\n",
    "    start_idx = None\n",
    "    for pat in _GB_START_MARKERS:\n",
    "        m = re.search(pat, t, flags=re.IGNORECASE)\n",
    "        if m:\n",
    "            # start AFTER the matched line\n",
    "            start_idx = t.find(\"\\n\", m.end())\n",
    "            if start_idx == -1:\n",
    "                start_idx = m.end()\n",
    "            break\n",
    "\n",
    "    # Find END\n",
    "    end_idx = None\n",
    "    for pat in _GB_END_MARKERS:\n",
    "        m = re.search(pat, t, flags=re.IGNORECASE)\n",
    "        if m:\n",
    "            # end BEFORE the matched line\n",
    "            end_idx = m.start()\n",
    "            break\n",
    "\n",
    "    if start_idx is not None and end_idx is not None and end_idx > start_idx:\n",
    "        core = t[start_idx:end_idx]\n",
    "    else:\n",
    "        # Fallback: try to start at first chapter-like heading\n",
    "        core = t\n",
    "        for pat in _CHAPTER_HINTS:\n",
    "            m = re.search(pat, core, flags=re.IGNORECASE | re.MULTILINE)\n",
    "            if m:\n",
    "                core = core[m.start():]\n",
    "                break\n",
    "        # And trim off the standard license tail if present\n",
    "        for pat in _GB_END_MARKERS:\n",
    "            m = re.search(pat, core, flags=re.IGNORECASE)\n",
    "            if m:\n",
    "                core = core[:m.start()]\n",
    "                break\n",
    "\n",
    "    # Remove license/contact blocks that sometimes sneak inside\n",
    "    core = re.sub(r\"\\n\\s*End of the Project Gutenberg.*\", \"\", core, flags=re.IGNORECASE)\n",
    "    core = re.sub(r\"\\*\\*\\*\\s*START: FULL LICENSE\\s*\\*\\*\\*.*\", \"\", core, flags=re.IGNORECASE | re.DOTALL)\n",
    "\n",
    "    # Clean leftover cruft: URLs, repeated separators\n",
    "    core = re.sub(r\"https?://\\S+\", \"\", core)\n",
    "    core = re.sub(r\"[ \\t]+\\n\", \"\\n\", core)   # trailing spaces before newline\n",
    "    core = re.sub(r\"\\n{3,}\", \"\\n\\n\", core)   # collapse big blank blocks\n",
    "    return core.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d2835e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_text(p: str) -> str:\n",
    "    with open(p, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "        return f.read()\n",
    "\n",
    "def normalize_text(t: str) -> str:\n",
    "    # 1) strip Gutenberg header/footer FIRST\n",
    "    # This removes license text and front/back matter that would otherwise pollute counts.\n",
    "    t = strip_gutenberg(t)\n",
    "    # 1.5) Normalize smart/curly apostrophes to ASCII apostrophe so contractions\n",
    "    # (e.g., don’t) are preserved as single tokens rather than split into\n",
    "    # two tokens (don t). This handles U+2019 (right single quotation mark) and\n",
    "    # U+2018 (left single quotation mark) commonly found in some texts/encodings.\n",
    "    t = t.replace(\"’\", \"'\")\n",
    "    t = t.replace(\"‘\", \"'\")\n",
    "    # 2) join hyphenated line breaks (e.g., \"won-\\nderful\")\n",
    "    t = re.sub(r\"-\\s*\\n\", \"\", t)\n",
    "    # 3) normalize whitespace\n",
    "    t = re.sub(r\"\\s+\", \" \", t)\n",
    "    return t\n",
    "\n",
    "text1 = normalize_text(load_text(CONFIG[\"book1_path\"]))\n",
    "text2 = normalize_text(load_text(CONFIG[\"book2_path\"]))\n",
    "\n",
    "tokens1 = WORD_RE.findall(text1.lower())\n",
    "tokens2 = WORD_RE.findall(text2.lower())\n",
    "\n",
    "if CONFIG[\"use_stopwords\"]:\n",
    "    tokens1 = [t for t in tokens1 if t not in STOPWORDS]\n",
    "    tokens2 = [t for t in tokens2 if t not in STOPWORDS]\n",
    "\n",
    "tokens = tokens1 + tokens2\n",
    "\n",
    "len(tokens1), len(tokens2), len(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0e1872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prune single-letter tokens except 'a' and 'i' to remove initials/artifacts\n",
    "# This prevents spurious n-grams like 'alice q s' coming from sequences such as 'Q.'s' or 'Q.’s'.\n",
    "keep_small = {'a', 'i'}\n",
    "tokens1 = [t for t in tokens1 if (len(t) > 1) or (t in keep_small)]\n",
    "tokens2 = [t for t in tokens2 if (len(t) > 1) or (t in keep_small)]\n",
    "# Recompute combined token sequence used elsewhere\n",
    "tokens = tokens1 + tokens2\n",
    "print('Lengths after prune: book1=', len(tokens1), 'book2=', len(tokens2), 'combined=', len(tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f18213",
   "metadata": {},
   "source": [
    "## 2. Helpers: n-gram generation & tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c345fa9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import tee, islice\n",
    "\n",
    "def ngrams(seq, n=2):\n",
    "    \"\"\"Return an iterator of n-grams from sequence `seq`.\n",
    "\n",
    "    Example: list(ngrams([1,2,3], n=2)) -> [(1,2),(2,3)]\n",
    "    \"\"\"\n",
    "    iters = tee(seq, n)\n",
    "    for i, it in enumerate(iters):\n",
    "        next(islice(it, i, i), None)\n",
    "    return zip(*iters)\n",
    "\n",
    "\n",
    "def freq_table(tokens, n=2, min_count=1, rate_base=10000, top_k=20):\n",
    "    \"\"\"Compute n-gram frequencies and return a DataFrame of top items.\n",
    "\n",
    "    Returns:\n",
    "      - df: pandas DataFrame with columns ['ngram','count','rate_per_X']\n",
    "      - ng: Counter of all n-grams\n",
    "      - total: total number of n-grams observed\n",
    "\n",
    "    The 'rate_per_X' column normalizes counts per `rate_base` tokens to compare\n",
    "    across books of different lengths.\n",
    "    \"\"\"\n",
    "    ng = Counter(ngrams(tokens, n))\n",
    "    total = sum(ng.values())\n",
    "    rows = []\n",
    "    for gram, c in ng.items():\n",
    "        if c >= min_count:\n",
    "            rows.append({\n",
    "                \"ngram\": \" \".join(gram),\n",
    "                \"count\": c,\n",
    "                \"rate_per_%d\" % rate_base: (c / max(1, total)) * rate_base\n",
    "            })\n",
    "    df = pd.DataFrame(rows).sort_values([\"count\", \"ngram\"], ascending=[False, True]).head(top_k)\n",
    "    return df, ng, total\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64eb245",
   "metadata": {},
   "source": [
    "## 3. Bigram Frequency (Combined & Per Book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81c4bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_c = CONFIG[\"min_ngram_count\"]\n",
    "top_k = CONFIG[\"top_k\"]\n",
    "rate_base = CONFIG[\"rate_base\"]\n",
    "\n",
    "# Combined\n",
    "bi_df, bi_counts, bi_total = freq_table(tokens, n=2, min_count=min_c, rate_base=rate_base, top_k=top_k)\n",
    "# Book-level\n",
    "bi_df_1, bi_counts_1, bi_total_1 = freq_table(tokens1, n=2, min_count=min_c, rate_base=rate_base, top_k=top_k)\n",
    "bi_df_2, bi_counts_2, bi_total_2 = freq_table(tokens2, n=2, min_count=min_c, rate_base=rate_base, top_k=top_k)\n",
    "\n",
    "bi_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778b9f5d",
   "metadata": {},
   "source": [
    "### 3.1 Plot Top Bigrams (Combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbab1924",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = bi_df.sort_values(\"count\").plot.barh(x=\"ngram\", y=\"count\", legend=False)\n",
    "ax.set_title(f\"Top Bigrams (min count ≥ {min_c}) — Combined\")\n",
    "# Keep a reference to the Matplotlib Figure so we can save it later.\n",
    "fig_combined = ax.get_figure()\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b801e73",
   "metadata": {},
   "source": [
    "### 3.2 Compare Distinctive Bigrams per Book\n",
    "\n",
    "We compute **relative rates** and show items that are strong in one book relative to the other.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f88c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge per-book bigram rates\n",
    "key = \"rate_per_%d\" % rate_base\n",
    "df1 = bi_df_1.rename(columns={\"count\":\"count_b1\", key:\"rate_b1\"})\n",
    "df2 = bi_df_2.rename(columns={\"count\":\"count_b2\", key:\"rate_b2\"})\n",
    "\n",
    "m = pd.merge(df1[[\"ngram\",\"count_b1\",\"rate_b1\"]],\n",
    "             df2[[\"ngram\",\"count_b2\",\"rate_b2\"]],\n",
    "             on=\"ngram\", how=\"outer\").fillna(0)\n",
    "\n",
    "# compute simple distinctiveness score (rate difference)\n",
    "m[\"rate_diff_b1_minus_b2\"] = m[\"rate_b1\"] - m[\"rate_b2\"]\n",
    "m[\"rate_diff_b2_minus_b1\"] = -m[\"rate_diff_b1_minus_b2\"]\n",
    "\n",
    "top_b1 = m.sort_values(\"rate_diff_b1_minus_b2\", ascending=False).head(top_k)\n",
    "top_b2 = m.sort_values(\"rate_diff_b2_minus_b1\", ascending=False).head(top_k)\n",
    "\n",
    "top_b1.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15567bd",
   "metadata": {},
   "source": [
    "### 3.3 Visualize Distinctive Bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04105432",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,2, figsize=(12,5), sharex=False)\n",
    "top_b1_plot = top_b1.sort_values(\"rate_diff_b1_minus_b2\").tail(10)\n",
    "axes[0].barh(top_b1_plot[\"ngram\"], top_b1_plot[\"rate_diff_b1_minus_b2\"])\n",
    "axes[0].set_title(\"Distinctive for Book 1 (rate diff)\")\n",
    "\n",
    "top_b2_plot = top_b2.sort_values(\"rate_diff_b2_minus_b1\").tail(10)\n",
    "axes[1].barh(top_b2_plot[\"ngram\"], top_b2_plot[\"rate_diff_b2_minus_b1\"])\n",
    "axes[1].set_title(\"Distinctive for Book 2 (rate diff)\")\n",
    "\n",
    "# Save reference to this figure for later export.\n",
    "fig_distinctive = fig\n",
    "\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241eb384",
   "metadata": {},
   "source": [
    "## 4. Trigram Frequency (Combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e64c5ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tri_df, tri_counts, tri_total = freq_table(tokens, n=3, min_count=min_c, rate_base=rate_base, top_k=top_k)\n",
    "tri_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40665efa",
   "metadata": {},
   "source": [
    "## Optional N-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bec77db",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_df, n_counts, n_total = freq_table(tokens, n=4, min_count=2, rate_base=rate_base, top_k=top_k)\n",
    "n_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27a826f7",
   "metadata": {},
   "source": [
    "You can see that most of the longer Ngrams come from the \"Jabberwocky\" song in Alice's story."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a7b317",
   "metadata": {},
   "source": [
    "### 4.1 Plot Top Trigrams (Combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53d3ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = tri_df.sort_values(\"count\").plot.barh(x=\"ngram\", y=\"count\", legend=False)\n",
    "ax.set_title(f\"Top Trigrams (min count ≥ {min_c}) — Combined\")\n",
    "# Keep fig reference so we can save explicitly later\n",
    "fig_tri = ax.get_figure()\n",
    "plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3ffd762",
   "metadata": {},
   "source": [
    "## 5. Reflection\n",
    "\n",
    "- Which bigrams/trigrams feel like your author’s “voice”?\n",
    "- Are the most frequent items also the most **distinctive** across your two works?\n",
    "- What changes when you toggle stopwords on/off?\n",
    "- If character names dominate, consider removing them and re-running.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae551c8",
   "metadata": {},
   "source": [
    "## 6. Export (tables/figures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a194b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(\"../results\").mkdir(exist_ok=True)\n",
    "\n",
    "# Save data tables\n",
    "bi_df.to_csv(\"../results/bigrams_combined.csv\", index=False)\n",
    "bi_df_1.to_csv(\"../results/bigrams_book1.csv\", index=False)\n",
    "bi_df_2.to_csv(\"../results/bigrams_book2.csv\", index=False)\n",
    "tri_df.to_csv(\"../results/trigrams_combined.csv\", index=False)\n",
    "\n",
    "# Save figures explicitly by referencing the Figure objects we created earlier.\n",
    "# This ensures we save the intended figures even after plt.show() was called.\n",
    "try:\n",
    "    fig_combined.savefig(\"../results/bigrams_plot.png\", dpi=200, bbox_inches=\"tight\")\n",
    "except NameError:\n",
    "    # fig_combined not defined (plot step may not have been executed)\n",
    "    try:\n",
    "        plt.savefig(\"../results/bigrams_plot.png\", dpi=200, bbox_inches=\"tight\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# Distinctive plots\n",
    "try:\n",
    "    fig_distinctive.savefig(\"../results/distinctive_bigrams_plot.png\", dpi=200, bbox_inches=\"tight\")\n",
    "except NameError:\n",
    "    try:\n",
    "        fig.savefig(\"../results/distinctive_bigrams_plot.png\", dpi=200, bbox_inches=\"tight\")\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "# Trigram figure\n",
    "try:\n",
    "    fig_tri.savefig(\"../results/trigrams_plot.png\", dpi=200, bbox_inches=\"tight\")\n",
    "except NameError:\n",
    "    try:\n",
    "        # fallback to last active figure\n",
    "        plt.savefig(\"../results/trigrams_plot.png\", dpi=200, bbox_inches=\"tight\")\n",
    "    except Exception:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a917e2e1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
